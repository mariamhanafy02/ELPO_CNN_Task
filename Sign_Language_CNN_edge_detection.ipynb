{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `Sign Language Detetcion using edge detection then CNN` ü™ü"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## import libraries üìö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XMN7d7lIMfE",
        "outputId": "5a3d8d38-08ae-4e7b-ad86-888efa5b727e"
      },
      "outputs": [],
      "source": [
        "#%pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GYtTPQaoH10g"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import os\n",
        "import cv2\n",
        "#import imghdr\n",
        "#import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.applications import ResNet50\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#----------------\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D, Dense,Flatten, Dropout\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attribute initialization üßö"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PMytDK-mFusw"
      },
      "outputs": [],
      "source": [
        "# Attributes Initialization\n",
        "labels_dir = 'Dataset\\Annotation'\n",
        "classes_dir= 'Dataset\\Signs_grayscale'\n",
        "image_exts = ['jpeg','jpg', 'bmp', 'png','JPG']                               #putting all the wanted image extensions\n",
        "class_names = ['drink', 'eat', 'hello','help','love','morning','okay', 'please','stop','thankyou']\n",
        "image_size = (224, 224)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use GPU üíª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "MkpMli07Ho5e"
      },
      "outputs": [],
      "source": [
        "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU30mohnHtce",
        "outputId": "6031e93f-9063-4aff-ab44-10363cd08b9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image PreprocessingüñºÔ∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "imageSize = (224, 224)\n",
        "batchSize = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## edge detetction on each image üçÉ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# import cv2\n",
        "# import os\n",
        "\n",
        "# # Set the root directory where the 10 folders are located\n",
        "# rootdir = 'Dataset\\Signs_grayscale'\n",
        "\n",
        "# # Set the lower and upper threshold values for the Canny edge detection\n",
        "# t_lower = 100\n",
        "# t_upper = 200\n",
        "\n",
        "# # Loop through the subdirectories of the root directory\n",
        "# for subdir, dirs, files in os.walk(rootdir):\n",
        "#     # Loop through the files in the current subdirectory\n",
        "#     for file in files:\n",
        "#         # Check if the file is an image (you can add more image file extensions if needed)\n",
        "#         if file.endswith(('.jpg', '.jpeg', '.png')):\n",
        "#             # Construct the full path to the image file\n",
        "#             filepath = os.path.join(subdir, file)\n",
        "            \n",
        "#             # Load the image using OpenCV\n",
        "#             img = cv2.imread(filepath)\n",
        "            \n",
        "#             # Apply Canny edge detection on the image\n",
        "#             edges = cv2.Canny(img, t_lower, t_upper)\n",
        "            \n",
        "#             # Save the resulting image (you can change the output directory and filename as needed)\n",
        "#             cv2.imwrite(filepath, edges)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split train and test ü™ì"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 942 files belonging to 10 classes.\n",
            "Using 754 files for training.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gen_data_train = tf.keras.utils.image_dataset_from_directory(\n",
        "    classes_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=class_names,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=batchSize,\n",
        "    image_size=imageSize,\n",
        "    shuffle=True,\n",
        "    seed=1,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 942 files belonging to 10 classes.\n",
            "Using 188 files for validation.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "gen_data_test = tf.keras.utils.image_dataset_from_directory(\n",
        "    classes_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=class_names,\n",
        "    color_mode='grayscale',\n",
        "    batch_size=batchSize,\n",
        "    image_size=imageSize,\n",
        "    shuffle=True,\n",
        "    seed=1,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN Model üì¶"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "#* CNN model\n",
        "classifier = tf.keras.Sequential()\n",
        "classifier.add(Conv2D(filters=16, kernel_size=(4, 4), strides=(1, 1), padding='same', input_shape=(224, 224, 1), activation='relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Conv2D(filters=16, kernel_size=(4, 4), strides=(1, 1), padding='same', activation='relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(16, activation='relu'))\n",
        "classifier.add(Dense(10, activation='softmax'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "24/24 [==============================] - 17s 670ms/step - loss: 14.1151 - accuracy: 0.0928\n",
            "Epoch 2/20\n",
            "24/24 [==============================] - 17s 680ms/step - loss: 2.3032 - accuracy: 0.0902\n",
            "Epoch 3/20\n",
            "24/24 [==============================] - 15s 626ms/step - loss: 2.3025 - accuracy: 0.0928\n",
            "Epoch 4/20\n",
            "24/24 [==============================] - 15s 597ms/step - loss: 2.3025 - accuracy: 0.0889\n",
            "Epoch 5/20\n",
            "24/24 [==============================] - 14s 574ms/step - loss: 2.3025 - accuracy: 0.0862\n",
            "Epoch 6/20\n",
            "24/24 [==============================] - 13s 554ms/step - loss: 2.3025 - accuracy: 0.1061\n",
            "Epoch 7/20\n",
            "24/24 [==============================] - 17s 706ms/step - loss: 2.3024 - accuracy: 0.1061\n",
            "Epoch 8/20\n",
            "24/24 [==============================] - 17s 682ms/step - loss: 2.3024 - accuracy: 0.1061\n",
            "Epoch 9/20\n",
            "24/24 [==============================] - 15s 637ms/step - loss: 2.3024 - accuracy: 0.1061\n",
            "Epoch 10/20\n",
            "24/24 [==============================] - 16s 647ms/step - loss: 2.3023 - accuracy: 0.1061\n",
            "Epoch 11/20\n",
            "24/24 [==============================] - 15s 621ms/step - loss: 2.3023 - accuracy: 0.1061\n",
            "Epoch 12/20\n",
            "24/24 [==============================] - 15s 606ms/step - loss: 2.3023 - accuracy: 0.1061\n",
            "Epoch 13/20\n",
            "24/24 [==============================] - 15s 604ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 14/20\n",
            "24/24 [==============================] - 14s 587ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 15/20\n",
            "24/24 [==============================] - 15s 620ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 16/20\n",
            "24/24 [==============================] - 17s 692ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 17/20\n",
            "24/24 [==============================] - 16s 657ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 18/20\n",
            "24/24 [==============================] - 17s 683ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 19/20\n",
            "24/24 [==============================] - 17s 710ms/step - loss: 2.3022 - accuracy: 0.1061\n",
            "Epoch 20/20\n",
            "24/24 [==============================] - 19s 763ms/step - loss: 2.3021 - accuracy: 0.1061\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x1cad8a645e0>"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "classifier.fit(gen_data_train, epochs=20, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 1s 85ms/step - loss: 2.3047 - accuracy: 0.0904\n",
            "Accuracy:  0.0904255285859108\n"
          ]
        }
      ],
      "source": [
        "accuracy = classifier.evaluate(gen_data_test,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 16)      272       \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 224, 224, 16)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 112, 112, 16)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 112, 112, 16)      4112      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 112, 112, 16)      0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 56, 56, 16)       0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 50176)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                802832    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                170       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 807,386\n",
            "Trainable params: 807,386\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## change to .h5 to .tflite üì≤"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# classifier.save(\"trained_model.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _update_step_xla while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\MARIAM~1\\AppData\\Local\\Temp\\tmpgk6ykz3k\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\MARIAM~1\\AppData\\Local\\Temp\\tmpgk6ykz3k\\assets\n"
          ]
        }
      ],
      "source": [
        "# # Load the trained Keras model from the file\n",
        "# loaded_model = keras.models.load_model(\"trained_model.h5\")\n",
        "# # Convert the model to TensorFlow Lite format\n",
        "# converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
        "# tflite_model = converter.convert()\n",
        "\n",
        "# # Save the TFLite model to a file\n",
        "# with open(\"trained_model.tflite\", \"wb\") as f:\n",
        "#     f.write(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
