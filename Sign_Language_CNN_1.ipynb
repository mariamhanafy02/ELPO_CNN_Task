{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# `Sign Language Predection using CNN`🪧🤘"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`Our approach:` 🧠🤯\n",
        "1. Collect the images \n",
        "2. Import Libraries\n",
        "3. Attribute Initialization\n",
        "4. Use gpu\n",
        "5. Image preprocessing\n",
        "   1. resizing image to (224,224)\n",
        "   1. splitting\n",
        "6. input to CNN model \n",
        "7. change model to .h5\n",
        "8. then to .tflite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XMN7d7lIMfE",
        "outputId": "5a3d8d38-08ae-4e7b-ad86-888efa5b727e"
      },
      "outputs": [],
      "source": [
        "#%pip install -U scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorflow\n",
            "  Using cached tensorflow-2.13.0-cp38-cp38-win_amd64.whl (1.9 kB)\n",
            "Collecting tensorflow-intel==2.13.0\n",
            "  Using cached tensorflow_intel-2.13.0-cp38-cp38-win_amd64.whl (276.5 MB)\n",
            "Requirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
            "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
            "  Using cached protobuf-4.23.4-cp38-cp38-win_amd64.whl (422 kB)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: libclang>=13.0.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (15.0.6.1)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.2.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.51.3)\n",
            "Collecting keras<2.14,>=2.13.1\n",
            "  Using cached keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: numpy<=1.24.3,>=1.22 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.24.3)\n",
            "Collecting tensorboard<2.14,>=2.13\n",
            "  Using cached tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
            "Requirement already satisfied: six>=1.12.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
            "Requirement already satisfied: packaging in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (21.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: setuptools in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (49.2.1)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in d:\\python\\python38\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\python\\python38\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.38.4)\n",
            "Collecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in d:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in d:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in d:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in d:\\python\\python38\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.16.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in d:\\python\\python38\\lib\\site-packages (from packaging->tensorflow-intel==2.13.0->tensorflow) (3.0.9)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in d:\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in d:\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in d:\\python\\python38\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in d:\\python\\python38\\lib\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in d:\\python\\python38\\lib\\site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in d:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.5.18.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in d:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in d:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in d:\\python\\python38\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.9)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\python\\python38\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
            "Requirement already satisfied: zipp>=0.5 in d:\\python\\python38\\lib\\site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in d:\\python\\python38\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in d:\\python\\python38\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
            "Installing collected packages: protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 3.19.6\n",
            "    Uninstalling protobuf-3.19.6:\n",
            "      Successfully uninstalled protobuf-3.19.6\n",
            "Successfully installed google-auth-oauthlib-1.0.0 keras-2.13.1 protobuf-4.23.4 tensorboard-2.13.0 tensorflow-2.13.0 tensorflow-intel-2.13.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: Ignoring invalid distribution -rotobuf (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (d:\\python\\python38\\lib\\site-packages)\n",
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "streamlit 1.17.0 requires protobuf<4,>=3.12, but you have protobuf 4.23.4 which is incompatible.\n",
            "mysql-connector-python 8.0.31 requires protobuf<=3.20.1,>=3.11.0, but you have protobuf 4.23.4 which is incompatible.\n",
            "WARNING: Ignoring invalid distribution -rotobuf (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -rotobuf (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution -ip (d:\\python\\python38\\lib\\site-packages)\n",
            "WARNING: Ignoring invalid distribution - (d:\\python\\python38\\lib\\site-packages)\n",
            "\n",
            "[notice] A new release of pip available: 22.3.1 -> 23.2.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "#%pip install tensorflow --user"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Libraries 📚"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "GYtTPQaoH10g"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import os\n",
        "import cv2\n",
        "#import imghdr\n",
        "#import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from keras.models import load_model\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.applications import ResNet50\n",
        "from sklearn.preprocessing import normalize\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "#----------------\n",
        "import keras\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "from matplotlib import pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D,MaxPooling2D, Dense,Flatten, Dropout\n",
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Attribute Initializing 📎"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "PMytDK-mFusw"
      },
      "outputs": [],
      "source": [
        "# Attributes Initialization\n",
        "labels_dir = 'Dataset\\Annotation'\n",
        "classes_dir= 'Dataset\\Signs'\n",
        "image_exts = ['jpeg','jpg', 'bmp', 'png','JPG']                               #putting all the wanted image extensions\n",
        "class_names = ['drink', 'eat', 'hello','help','love','morning','okay', 'please','stop','thankyou']\n",
        "image_size = (224, 224)\n",
        "batch_size = 32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the GPU 💻"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MkpMli07Ho5e"
      },
      "outputs": [],
      "source": [
        "# Avoid OOM errors by setting GPU Memory Consumption Growth\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "for gpu in gpus:\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UU30mohnHtce",
        "outputId": "6031e93f-9063-4aff-ab44-10363cd08b9e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import glob\n",
        "# import shutil\n",
        "# import os\n",
        "# import xml.etree.ElementTree as ET"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# #for each class name loop on annotation find its equivalent label then find this file in images and add it to its file name \n",
        "# for x in class_names:\n",
        "#   directory = labels_dir\n",
        "#   print(\"class:\",x)\n",
        "#   # iterate over files in\n",
        "#   # that directory\n",
        "#   for filename in os.listdir(directory):\n",
        "#       image_jpg=os.path.splitext(filename)[0]\n",
        "#       print(image_jpg)\n",
        "#       f = os.path.join(directory, filename)\n",
        "#       # checking if it is a file\n",
        "#       if os.path.isfile(f):\n",
        "#           print(\"xml file :\",f)\n",
        "#           tree = ET.parse(f)\n",
        "#           root = tree.getroot()\n",
        "#           tag_name=root[6][0].text\n",
        "#           print(\"tag:\",tag_name)\n",
        "#           file_name=image_jpg + \".jpg\"\n",
        "#           #extract the label\n",
        "#           if (x==tag_name):\n",
        "#             #get the file from images move to signs\\class\n",
        "#             src_dir=os.path.join(data_dir, file_name)\n",
        "#             dst_dir = os.path.join(classes_dir, x)\n",
        "#             print(\"al image gaya menan:\",src_dir)\n",
        "#             print(\"al image ray7a fan:\",dst_dir)\n",
        "#             shutil.copy(src_dir,dst_dir)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Image Preprocessing🖼️"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "imageSize = (224, 224)\n",
        "batchSize = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "LkW_clMlIV2h",
        "outputId": "920a870e-ddbc-469e-c2eb-1db001937b0c"
      },
      "outputs": [],
      "source": [
        "# # we define an ImageDataGenerator object and\n",
        "# # * set rescale to 1./255 to rescale the pixel values to the range [0, 1].\n",
        "# data_generator = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "# # we use the flow_from_directory method of the\n",
        "# # ImageDataGenerator object to load a subset of the dataset\n",
        "# # * we made resizing and image interpolation using bilinear\n",
        "# gen_data = data_generator.flow_from_directory(\n",
        "#     directory=classes_dir,\n",
        "#     target_size=(224, 224),\n",
        "#     batch_size=32, \n",
        "#     class_mode=None,\n",
        "#     shuffle=False,\n",
        "#     interpolation='bilinear'\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split Train and Test 🪓"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 942 files belonging to 10 classes.\n",
            "Using 754 files for training.\n"
          ]
        }
      ],
      "source": [
        "gen_data_train =tf.keras.utils.image_dataset_from_directory(\n",
        "    classes_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=class_names,\n",
        "    color_mode='rgb',\n",
        "    batch_size=batchSize,\n",
        "    image_size=imageSize,\n",
        "    shuffle=True,\n",
        "    seed=3,\n",
        "    validation_split=0.2,\n",
        "    subset='training',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        "\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 942 files belonging to 10 classes.\n",
            "Using 188 files for validation.\n"
          ]
        }
      ],
      "source": [
        "gen_data_test =tf.keras.utils.image_dataset_from_directory(\n",
        "    classes_dir,\n",
        "    labels='inferred',\n",
        "    label_mode='categorical',\n",
        "    class_names=class_names,\n",
        "    color_mode='rgb',\n",
        "    batch_size=batchSize,\n",
        "    image_size=imageSize,\n",
        "    shuffle=True,\n",
        "    seed=3,\n",
        "    validation_split=0.2,\n",
        "    subset='validation',\n",
        "    interpolation='bilinear',\n",
        "    follow_links=False,\n",
        "    crop_to_aspect_ratio=False,\n",
        "\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#print(gen_data_test[0])\n",
        "#print(gen_data_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pe7DjiWuIleQ"
      },
      "outputs": [],
      "source": [
        "# #visualization\n",
        "# fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(10,10))\n",
        "# for i in range(4):\n",
        "\n",
        "#   # convert to unsigned integers for plotting\n",
        "#   image = next(gen_data)[6]\n",
        "\n",
        "#   # changing size from (1, 200, 200, 3) to (200, 200, 3) for plotting the image\n",
        "#   image = np.squeeze(image)\n",
        "\n",
        "#   # plot raw pixel data\n",
        "#   ax[i].imshow(image)\n",
        "#   ax[i].axis('off')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## CNN model 📦"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "#* CNN model\n",
        "classifier = Sequential()\n",
        "classifier.add(Conv2D(filters=32, kernel_size=(4,4),strides=(1,1),padding='same',input_shape=(224, 224, 3),activation='relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "classifier.add(Conv2D(filters=64, kernel_size=(4,4),strides=(1,1),padding='same',activation='relu'))\n",
        "classifier.add(Dropout(0.5))\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "classifier.add(Flatten())\n",
        "classifier.add(Dense(128, activation='relu'))\n",
        "classifier.add(Dense(10, activation='softmax'))\n",
        "classifier.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            " 4/24 [====>.........................] - ETA: 33s - loss: 7266.7588 - accuracy: 0.1094"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m classifier\u001b[39m.\u001b[39mcompile(loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcategorical_crossentropy\u001b[39m\u001b[39m'\u001b[39m, optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m----> 2\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(gen_data_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\keras\\src\\engine\\training.py:1742\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1734\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[0;32m   1735\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   1736\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1739\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[0;32m   1740\u001b[0m ):\n\u001b[0;32m   1741\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m-> 1742\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[0;32m   1743\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[0;32m   1744\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:825\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    822\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    824\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 825\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[0;32m    827\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    828\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:857\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    854\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    855\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    856\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 857\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    859\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    860\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[0;32m    861\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:148\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    146\u001b[0m   (concrete_function,\n\u001b[0;32m    147\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m--> 148\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m    149\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\monomorphic_function.py:1349\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs)\u001b[0m\n\u001b[0;32m   1345\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1346\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1347\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1348\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1349\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function(\u001b[39m*\u001b[39;49margs))\n\u001b[0;32m   1350\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1351\u001b[0m     args,\n\u001b[0;32m   1352\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1353\u001b[0m     executing_eagerly)\n\u001b[0;32m   1354\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:196\u001b[0m, in \u001b[0;36mAtomicFunction.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    194\u001b[0m \u001b[39mwith\u001b[39;00m record\u001b[39m.\u001b[39mstop_recording():\n\u001b[0;32m    195\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_bound_context\u001b[39m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 196\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_bound_context\u001b[39m.\u001b[39;49mcall_function(\n\u001b[0;32m    197\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname,\n\u001b[0;32m    198\u001b[0m         \u001b[39mlist\u001b[39;49m(args),\n\u001b[0;32m    199\u001b[0m         \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunction_type\u001b[39m.\u001b[39;49mflat_outputs),\n\u001b[0;32m    200\u001b[0m     )\n\u001b[0;32m    201\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    202\u001b[0m     outputs \u001b[39m=\u001b[39m make_call_op_in_graph(\u001b[39mself\u001b[39m, \u001b[39mlist\u001b[39m(args))\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1457\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1455\u001b[0m cancellation_context \u001b[39m=\u001b[39m cancellation\u001b[39m.\u001b[39mcontext()\n\u001b[0;32m   1456\u001b[0m \u001b[39mif\u001b[39;00m cancellation_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m-> 1457\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m   1458\u001b[0m       name\u001b[39m.\u001b[39;49mdecode(\u001b[39m\"\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1459\u001b[0m       num_outputs\u001b[39m=\u001b[39;49mnum_outputs,\n\u001b[0;32m   1460\u001b[0m       inputs\u001b[39m=\u001b[39;49mtensor_inputs,\n\u001b[0;32m   1461\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m   1462\u001b[0m       ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1463\u001b[0m   )\n\u001b[0;32m   1464\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1465\u001b[0m   outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1466\u001b[0m       name\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[0;32m   1467\u001b[0m       num_outputs\u001b[39m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1471\u001b[0m       cancellation_manager\u001b[39m=\u001b[39mcancellation_context,\n\u001b[0;32m   1472\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     55\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "classifier.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "classifier.fit(gen_data_train, epochs=20, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6/6 [==============================] - 1s 165ms/step - loss: 122.6270 - accuracy: 0.1064\n",
            "Accuracy:  0.10638298094272614\n"
          ]
        }
      ],
      "source": [
        "accuracy = classifier.evaluate(gen_data_test,batch_size=32)\n",
        "print(\"Accuracy: \",accuracy[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d (Conv2D)             (None, 224, 224, 32)      1568      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 224, 224, 32)      0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2  (None, 112, 112, 32)      0         \n",
            " D)                                                              \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 112, 112, 64)      32832     \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPoolin  (None, 56, 56, 64)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 200704)            0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               25690240  \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 25725930 (98.14 MB)\n",
            "Trainable params: 25725930 (98.14 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "classifier.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save model to .h5 📱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\lenovo\\anaconda3\\envs\\Elpo\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "classifier.save(\"trained_model.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Model to .tflite 📲"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\AppData\\Local\\Temp\\tmppv6xvqnt\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: C:\\Users\\lenovo\\AppData\\Local\\Temp\\tmppv6xvqnt\\assets\n"
          ]
        }
      ],
      "source": [
        "# Load the trained Keras model from the file\n",
        "loaded_model = keras.models.load_model(\"trained_model.h5\")\n",
        "# Convert the model to TensorFlow Lite format\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
        "tflite_model = converter.convert()\n",
        "\n",
        "# Save the TFLite model to a file\n",
        "with open(\"trained_model.tflite\", \"wb\") as f:\n",
        "    f.write(tflite_model)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
