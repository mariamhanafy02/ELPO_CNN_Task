{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6980e52",
   "metadata": {},
   "source": [
    "# `Sign Language Prediction using ResNet` ðŸ—ƒï¸â˜‘ï¸"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7185b944",
   "metadata": {},
   "source": [
    "## import libraries ðŸ“š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4f4dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import os\n",
    "import cv2\n",
    "#import imghdr\n",
    "#import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.applications import ResNet50\n",
    "from sklearn.preprocessing import normalize\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "#----------------\n",
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D, Dense,Flatten, Dropout\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "400b6d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "\n",
    "# high_level_directory = 'C:/Users/HP/Desktop/Dataset_for_CNN_ELPO'  # Specify the path to your directory here\n",
    "\n",
    "# # Get a list of all files in the directory\n",
    "# class_list = os.listdir(high_level_directory)\n",
    "# x_path=[]\n",
    "# y_labels=[]\n",
    "# # Iterate over the file list and print each file name\n",
    "# for c_name in class_list:\n",
    "#     class_path=high_level_directory + \"/\" +c_name\n",
    "#     images_names=os.listdir(class_path)\n",
    "#     for image in images_names:\n",
    "#         image_path=class_path+image\n",
    "#         x_path.append(image_path)\n",
    "#         y_labels.append(c_name)\n",
    "#     #print(image_path)\n",
    "#     #print(class_path)\n",
    "#     #print(c_name)\n",
    "###################################################################################33\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "data_path = 'Dataset\\Signs'\n",
    "high_level_directory = os.listdir(data_path)\n",
    "\n",
    "# List to store image paths and corresponding labels\n",
    "images = []\n",
    "labels = []\n",
    "classes=[]\n",
    "\n",
    "for class_name in high_level_directory:\n",
    "    classes.append(class_name)\n",
    "    class_path = os.path.join(data_path, class_name)\n",
    "    for image_name in os.listdir(class_path):\n",
    "        image_path = os.path.join(class_path, image_name)\n",
    "        img = cv2.imread(image_path)\n",
    "        resized_img=cv2.resize(img,(224,224))\n",
    "        images.append(resized_img)\n",
    "        labels.append(class_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6203e6f3",
   "metadata": {},
   "source": [
    "## Split data test and train ðŸª“"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e203fdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(images, labels,stratify=labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7f7a56da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(224, 224, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "955ea8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72ac6126",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "753"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9669f4c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcc51c9",
   "metadata": {},
   "source": [
    "## ResNet Model creation ðŸ§š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7de7fa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "540dd950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 942 files belonging to 10 classes.\n",
      "Using 754 files for training.\n",
      "Using 188 files for validation.\n"
     ]
    }
   ],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "train_dir = data_path\n",
    "\n",
    "train_data, test_data = tf.keras.preprocessing.image_dataset_from_directory(train_dir,\n",
    "                                                                                label_mode=\"categorical\",\n",
    "                                                                                image_size=IMG_SIZE,\n",
    "                                                                                validation_split=0.2,\n",
    "                                                                                seed = 42,\n",
    "                                                                                batch_size=32,\n",
    "                                                                                subset=\"both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "acab0313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb0_notop.h5\n",
      "16705208/16705208 [==============================] - 5s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "\n",
    "# Create base model\n",
    "input_shape = (224, 224, 3)\n",
    "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
    "base_model.trainable = False # freeze base model layers\n",
    "\n",
    "# Create Functional model \n",
    "inputs = layers.Input(shape=input_shape, name=\"input_layer\")\n",
    "# Note: EfficientNetBX models have rescaling built-in but if your model didn't you could have a layer like below\n",
    "# x = layers.Rescaling(1./255)(x)\n",
    "x = base_model(inputs, training=False) # set base_model to inference mode only\n",
    "x = layers.GlobalAveragePooling2D(name=\"pooling_layer\")(x)\n",
    "x = layers.Dense(len(classes))(x) # want one output neuron per class \n",
    "# Separate activation of output layer so we can output float32 activations\n",
    "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name=\"softmax_float32\")(x) \n",
    "model = tf.keras.Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=\"categorical_crossentropy\", # Use sparse_categorical_crossentropy when labels are *not* one-hot\n",
    "              optimizer=tf.keras.optimizers.Adam(),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5e7f67a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 224, 224, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetb0 (Functional  (None, None, None, 1280   4049571   \n",
      " )                           )                                   \n",
      "                                                                 \n",
      " pooling_layer (GlobalAvera  (None, 1280)              0         \n",
      " gePooling2D)                                                    \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                12810     \n",
      "                                                                 \n",
      " softmax_float32 (Activatio  (None, 10)                0         \n",
      " n)                                                              \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4062381 (15.50 MB)\n",
      "Trainable params: 12810 (50.04 KB)\n",
      "Non-trainable params: 4049571 (15.45 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d093f56d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "24/24 [==============================] - 29s 983ms/step - loss: 2.0960 - accuracy: 0.2546\n",
      "Epoch 2/20\n",
      "24/24 [==============================] - 28s 1s/step - loss: 1.5989 - accuracy: 0.6273\n",
      "Epoch 3/20\n",
      "24/24 [==============================] - 27s 1s/step - loss: 1.2790 - accuracy: 0.7427\n",
      "Epoch 4/20\n",
      "24/24 [==============================] - 25s 1s/step - loss: 1.0781 - accuracy: 0.7958\n",
      "Epoch 5/20\n",
      "24/24 [==============================] - 26s 1s/step - loss: 0.9250 - accuracy: 0.8448\n",
      "Epoch 6/20\n",
      "24/24 [==============================] - 25s 1000ms/step - loss: 0.8083 - accuracy: 0.8607\n",
      "Epoch 7/20\n",
      "24/24 [==============================] - 26s 1s/step - loss: 0.7246 - accuracy: 0.8727\n",
      "Epoch 8/20\n",
      "24/24 [==============================] - 21s 875ms/step - loss: 0.6487 - accuracy: 0.9125\n",
      "Epoch 9/20\n",
      "24/24 [==============================] - 21s 858ms/step - loss: 0.5838 - accuracy: 0.9271\n",
      "Epoch 10/20\n",
      "24/24 [==============================] - 21s 859ms/step - loss: 0.5361 - accuracy: 0.9324\n",
      "Epoch 11/20\n",
      "24/24 [==============================] - 21s 847ms/step - loss: 0.4934 - accuracy: 0.9416\n",
      "Epoch 12/20\n",
      "24/24 [==============================] - 21s 852ms/step - loss: 0.4554 - accuracy: 0.9496\n",
      "Epoch 13/20\n",
      "24/24 [==============================] - 21s 850ms/step - loss: 0.4185 - accuracy: 0.9629\n",
      "Epoch 14/20\n",
      "24/24 [==============================] - 21s 848ms/step - loss: 0.3915 - accuracy: 0.9655\n",
      "Epoch 15/20\n",
      "24/24 [==============================] - 22s 927ms/step - loss: 0.3652 - accuracy: 0.9668\n",
      "Epoch 16/20\n",
      "24/24 [==============================] - 22s 886ms/step - loss: 0.3412 - accuracy: 0.9708\n",
      "Epoch 17/20\n",
      "24/24 [==============================] - 22s 885ms/step - loss: 0.3197 - accuracy: 0.9721\n",
      "Epoch 18/20\n",
      "24/24 [==============================] - 22s 886ms/step - loss: 0.3019 - accuracy: 0.9788\n",
      "Epoch 19/20\n",
      "24/24 [==============================] - 22s 886ms/step - loss: 0.2862 - accuracy: 0.9775\n",
      "Epoch 20/20\n",
      "24/24 [==============================] - 22s 885ms/step - loss: 0.2693 - accuracy: 0.9828\n"
     ]
    }
   ],
   "source": [
    "# Turn off all warnings except for errors\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Fit the model with callbacks\n",
    "history_101_food_classes_feature_extract = model.fit(train_data, \n",
    "                                                     epochs=20,\n",
    "                                                     steps_per_epoch=len(train_data),\n",
    "                                                     validation_data=test_data,\n",
    "                                                     validation_steps=int(0.15 * len(test_data)),)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "713301d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sequence length: 224\n"
     ]
    }
   ],
   "source": [
    "max_length = 0\n",
    "\n",
    "# Iterate through the sequences in X_test to find the maximum length\n",
    "for sequence in X_test:\n",
    "    length = len(sequence)\n",
    "    if length > max_length:\n",
    "        max_length = length\n",
    "\n",
    "print(\"Maximum sequence length:\", max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "acf5828c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 [==============================] - 7s 929ms/step - loss: 0.5880 - accuracy: 0.8351\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.5879871845245361, 0.835106372833252]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_model = model.evaluate(test_data)\n",
    "result_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7bbbe6b",
   "metadata": {},
   "source": [
    "## Save model as .h5 ðŸ“±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48ecd904",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mariam Barakat\\Desktop\\work\\ELPO\\CNN_Task3\\The_final_project_files\\ELPO_CNN_Task\\Sign_Language_CNN_RESNET50.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mariam%20Barakat/Desktop/work/ELPO/CNN_Task3/The_final_project_files/ELPO_CNN_Task/Sign_Language_CNN_RESNET50.ipynb#X25sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39;49msave(\u001b[39m\"\u001b[39;49m\u001b[39mtrained_model.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32md:\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\python\\python38\\lib\\json\\__init__.py:234\u001b[0m, in \u001b[0;36mdumps\u001b[1;34m(obj, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m JSONEncoder\n\u001b[1;32m--> 234\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mcls\u001b[39;49m(\n\u001b[0;32m    235\u001b[0m     skipkeys\u001b[39m=\u001b[39;49mskipkeys, ensure_ascii\u001b[39m=\u001b[39;49mensure_ascii,\n\u001b[0;32m    236\u001b[0m     check_circular\u001b[39m=\u001b[39;49mcheck_circular, allow_nan\u001b[39m=\u001b[39;49mallow_nan, indent\u001b[39m=\u001b[39;49mindent,\n\u001b[0;32m    237\u001b[0m     separators\u001b[39m=\u001b[39;49mseparators, default\u001b[39m=\u001b[39;49mdefault, sort_keys\u001b[39m=\u001b[39;49msort_keys,\n\u001b[0;32m    238\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\u001b[39m.\u001b[39;49mencode(obj)\n",
      "File \u001b[1;32md:\\python\\python38\\lib\\json\\encoder.py:199\u001b[0m, in \u001b[0;36mJSONEncoder.encode\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[39mreturn\u001b[39;00m encode_basestring(o)\n\u001b[0;32m    196\u001b[0m \u001b[39m# This doesn't pass the iterator directly to ''.join() because the\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[39m# exceptions aren't as detailed.  The list call should be roughly\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# equivalent to the PySequence_Fast that ''.join() would do.\u001b[39;00m\n\u001b[1;32m--> 199\u001b[0m chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterencode(o, _one_shot\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m    200\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(chunks, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[0;32m    201\u001b[0m     chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks)\n",
      "File \u001b[1;32md:\\python\\python38\\lib\\json\\encoder.py:257\u001b[0m, in \u001b[0;36mJSONEncoder.iterencode\u001b[1;34m(self, o, _one_shot)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    253\u001b[0m     _iterencode \u001b[39m=\u001b[39m _make_iterencode(\n\u001b[0;32m    254\u001b[0m         markers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdefault, _encoder, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindent, floatstr,\n\u001b[0;32m    255\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkey_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitem_separator, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msort_keys,\n\u001b[0;32m    256\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mskipkeys, _one_shot)\n\u001b[1;32m--> 257\u001b[0m \u001b[39mreturn\u001b[39;00m _iterencode(o, \u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Unable to serialize [2.0896919 2.1128857 2.1081853] to JSON. Unrecognized type <class 'tensorflow.python.framework.ops.EagerTensor'>."
     ]
    }
   ],
   "source": [
    "model.save(\"trained_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65b852a",
   "metadata": {},
   "source": [
    "## Save to .tflite ðŸ“²"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e53859b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x0000025A850F8130>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Mariam Barakat\\Desktop\\work\\ELPO\\CNN_Task3\\The_final_project_files\\ELPO_CNN_Task\\Sign_Language_CNN_RESNET50.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariam%20Barakat/Desktop/work/ELPO/CNN_Task3/The_final_project_files/ELPO_CNN_Task/Sign_Language_CNN_RESNET50.ipynb#X26sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Load the trained Keras model from the file\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Mariam%20Barakat/Desktop/work/ELPO/CNN_Task3/The_final_project_files/ELPO_CNN_Task/Sign_Language_CNN_RESNET50.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m loaded_model \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mmodels\u001b[39m.\u001b[39;49mload_model(\u001b[39m\"\u001b[39;49m\u001b[39mtrained_model.h5\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariam%20Barakat/Desktop/work/ELPO/CNN_Task3/The_final_project_files/ELPO_CNN_Task/Sign_Language_CNN_RESNET50.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Convert the model to TensorFlow Lite format\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Mariam%20Barakat/Desktop/work/ELPO/CNN_Task3/The_final_project_files/ELPO_CNN_Task/Sign_Language_CNN_RESNET50.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m converter \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mlite\u001b[39m.\u001b[39mTFLiteConverter\u001b[39m.\u001b[39mfrom_keras_model(loaded_model)\n",
      "File \u001b[1;32md:\\python\\python38\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\python\\python38\\lib\\site-packages\\keras\\saving\\legacy\\hdf5_format.py:188\u001b[0m, in \u001b[0;36mload_model_from_hdf5\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    186\u001b[0m model_config \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mattrs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mmodel_config\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    187\u001b[0m \u001b[39mif\u001b[39;00m model_config \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 188\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    189\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNo model config found in the file at \u001b[39m\u001b[39m{\u001b[39;00mfilepath\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    190\u001b[0m     )\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(model_config, \u001b[39m\"\u001b[39m\u001b[39mdecode\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    192\u001b[0m     model_config \u001b[39m=\u001b[39m model_config\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: No model config found in the file at <tensorflow.python.platform.gfile.GFile object at 0x0000025A850F8130>."
     ]
    }
   ],
   "source": [
    "# Load the trained Keras model from the file\n",
    "loaded_model = keras.models.load_model(\"trained_model.h5\")\n",
    "# Convert the model to TensorFlow Lite format\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TFLite model to a file\n",
    "with open(\"trained_model.tflite\", \"wb\") as f:\n",
    "    f.write(tflite_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
